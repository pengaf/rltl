{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def clamp(value, min_value, max_value):\n",
    "    return max(min(value, max_value), min_value)\n",
    "\n",
    "class MountainCar:\n",
    "    def __init__(self,  maxStep=500):\n",
    "        self.maxStep = maxStep\n",
    "        self.curStep = 0\n",
    "        self.pxbound = (-1.2, 0.5)\n",
    "        self.vxbound = (-0.07, 0.07)\n",
    "\n",
    "    def state_space(self):\n",
    "        return (self.pxbound, self.vxbound)\n",
    "    \n",
    "    def action_shape(self):\n",
    "        return 3\n",
    "    \n",
    "    def reset(self):\n",
    "        self.curStep = 0\n",
    "        self.px = random.random()*0.2 - 0.6        \n",
    "        self.vx = 0\n",
    "        return (self.px, self.vx), \"\"\n",
    "    \n",
    "    def step(self, action):\n",
    "        #assert(0 <= action and action <=2)\n",
    "        acc = (action - 1.0)*0.001\n",
    "        self.vx = clamp(self.vx + acc - 0.0025*math.cos(3 * self.px), *self.vxbound)\n",
    "        self.px = clamp(self.px + self.vx, *self.pxbound)\n",
    "        if self.px == self.pxbound[0]:\n",
    "            self.vx = 0\n",
    "        terminated = self.px == self.pxbound[1]\n",
    "        self.curStep += 1\n",
    "        truncated = self.curStep >= self.maxStep\n",
    "        return (self.px, self.vx), -1, terminated, truncated, \"\"\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, terminated):\n",
    "        self.buffer.append((state, action, reward, next_state, terminated))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, terminateds = zip(*transitions)\n",
    "        return np.array(states), actions, rewards, np.array(next_states), terminateds\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "\n",
    "class Qnet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        #self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        #self.fc3 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        #x = torch.nn.functional.relu(self.fc2(x))\n",
    "        #x = torch.nn.functional.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "    \n",
    "class DQN:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, epsilon, target_update, device):\n",
    "        self.action_dim = action_dim\n",
    "        self.q_net = Qnet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_net = Qnet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(),lr = learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.target_update = target_update\n",
    "        self.count = 0\n",
    "        self.device = device\n",
    "        \n",
    "    def take_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            #print(state)\n",
    "            state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "            #print(\"state.shape\", state.shape)\n",
    "            value = self.q_net(state)\n",
    "            #print(\"value.shape\",value.shape)\n",
    "            argmax = value.argmax()\n",
    "            #print(\"argmax.shape\",argmax.shape)\n",
    "            action = self.q_net(state).argmax().item()\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def update(self, states, actions, rewards, next_states, terminateds):\n",
    "        states = torch.tensor(states,dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(actions,dtype=torch.int64).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(rewards,dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(next_states,dtype=torch.float).to(self.device)\n",
    "        terminateds = torch.tensor(terminateds,dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "        #with torch.no_grad():\n",
    "        max_next_q_values = self.target_net(next_states).max(1)[0].view(-1, 1)\n",
    "        #print(type(max_next_q_values))\n",
    "        #print(rewards.shape, max_next_q_values.shape, terminateds.shape)\n",
    "        q_targets = rewards + self.gamma * max_next_q_values * (1.0 - terminateds)\n",
    "\n",
    "        dqn_loss = torch.mean(torch.nn.functional.mse_loss(q_values, q_targets))\n",
    "        self.optimizer.zero_grad()\n",
    "        dqn_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.count += 1\n",
    "\n",
    "\n",
    "hidden_dim = 8192\n",
    "\n",
    "lr = 1e-3\n",
    "gamma = 0.98\n",
    "epsilon = 0.1\n",
    "target_update = 5\n",
    "buffer_size = 10000\n",
    "minimal_size = 500\n",
    "learn_freq = 5\n",
    "batch_size = 64\n",
    "device = torch.device(\"cpu\") #torch.device(\"cuda\") if torch.cuda.is_available () else torch.device(\"cpu\")#\n",
    "\n",
    "state_dim = 2\n",
    "action_dim = 3\n",
    "\n",
    "def train(agent, num_episodes):    \n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "    reward_stat = np.zeros(num_episodes)\n",
    "    total_steps = 0\n",
    "    env = MountainCar()\n",
    "    start_time = time.time()\n",
    "    max_reward = -100000 \n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            total_steps += 1\n",
    "            action = agent.take_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            replay_buffer.add(state, action, reward, next_state, terminated)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if replay_buffer.size() > minimal_size:\n",
    "                if total_steps % learn_freq ==0:\n",
    "                    states, actions, rewards, next_states, terminateds = replay_buffer.sample(batch_size)\n",
    "                    agent.update(states, actions, rewards, next_states, terminateds)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        new_max_reard = max_reward < total_reward\n",
    "        if new_max_reard:\n",
    "            max_reward = total_reward\n",
    "        if new_max_reard or episode*10 % num_episodes == 0:\n",
    "            duration = time.time() - start_time\n",
    "            steps_per_second = total_steps/duration\n",
    "            print(\"episode:\", episode, \"total_reward:\",total_reward, \"step/second:\", steps_per_second)\n",
    "        reward_stat[episode] = total_reward\n",
    "    return total_steps,reward_stat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PyTorch is not linked with support for vulkan devices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mg:\\pengaf\\rltl\\test\\test_dqn.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_dqn.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m agent \u001b[39m=\u001b[39m DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon, target_update, device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_dqn.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m total_steps, reward_stat \u001b[39m=\u001b[39m train(agent, \u001b[39m1000\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_dqn.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(total_steps)\n",
      "\u001b[1;32mg:\\pengaf\\rltl\\test\\test_dqn.ipynb Cell 2\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_dqn.ipynb#W1sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, state_dim, hidden_dim, action_dim, learning_rate, gamma, epsilon, target_update, device):\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_dqn.ipynb#W1sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dim \u001b[39m=\u001b[39m action_dim\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_dqn.ipynb#W1sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net \u001b[39m=\u001b[39m Qnet(state_dim, hidden_dim, action_dim)\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_dqn.ipynb#W1sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_net \u001b[39m=\u001b[39m Qnet(state_dim, hidden_dim, action_dim)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_dqn.ipynb#W1sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net\u001b[39m.\u001b[39mparameters(),lr \u001b[39m=\u001b[39m learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 833\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    834\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    835\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m   1156\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1158\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: PyTorch is not linked with support for vulkan devices"
     ]
    }
   ],
   "source": [
    "agent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon, target_update, device)\n",
    "total_steps, reward_stat = train(agent, 1000)\n",
    "print(total_steps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
