{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def clamp(value, min_value, max_value):\n",
    "    return max(min(value, max_value), min_value)\n",
    "\n",
    "class MountainCar:\n",
    "    def __init__(self,  maxStep=500):\n",
    "        self.maxStep = maxStep\n",
    "        self.curStep = 0\n",
    "        self.pxbound = (-1.2, 0.5)\n",
    "        self.vxbound = (-0.07, 0.07)\n",
    "\n",
    "    def state_space(self):\n",
    "        return (self.pxbound, self.vxbound)\n",
    "    \n",
    "    def action_shape(self):\n",
    "        return 3\n",
    "    \n",
    "    def reset(self):\n",
    "        self.curStep = 0\n",
    "        self.px = random.random()*0.2 - 0.6        \n",
    "        self.vx = 0\n",
    "        return (self.px, self.vx), \"\"\n",
    "    \n",
    "    def step(self, action):\n",
    "        #assert(0 <= action and action <=2)\n",
    "        acc = (action - 1.0)*0.001\n",
    "        self.vx = clamp(self.vx + acc - 0.0025*math.cos(3 * self.px), *self.vxbound)\n",
    "        self.px = clamp(self.px + self.vx, *self.pxbound)\n",
    "        if self.px == self.pxbound[0]:\n",
    "            self.vx = 0\n",
    "        terminated = self.px == self.pxbound[1]\n",
    "        self.curStep += 1\n",
    "        truncated = self.curStep >= self.maxStep\n",
    "        return (self.px, self.vx), -1, terminated, truncated, \"\"\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, terminated):\n",
    "        self.buffer.append((state, action, reward, next_state, terminated))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, terminateds = zip(*transitions)\n",
    "        return np.array(states), actions, rewards, np.array(next_states), terminateds\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "\n",
    "class Qnet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        #self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        #self.fc3 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        #x = torch.nn.functional.relu(self.fc2(x))\n",
    "        #x = torch.nn.functional.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "    \n",
    "class DQN:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, epsilon, target_update, device):\n",
    "        self.action_dim = action_dim\n",
    "        self.q_net = Qnet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_net = Qnet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(),lr = learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.target_update = target_update\n",
    "        self.count = 0\n",
    "        self.device = device\n",
    "        \n",
    "    def take_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            #print(state)\n",
    "            state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "            #print(\"state.shape\", state.shape)\n",
    "            value = self.q_net(state)\n",
    "            #print(\"value.shape\",value.shape)\n",
    "            argmax = value.argmax()\n",
    "            #print(\"argmax.shape\",argmax.shape)\n",
    "            action = self.q_net(state).argmax().item()\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def update(self, states, actions, rewards, next_states, terminateds):\n",
    "        states = torch.tensor(states,dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(actions,dtype=torch.int64).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(rewards,dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(next_states,dtype=torch.float).to(self.device)\n",
    "        terminateds = torch.tensor(terminateds,dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "        #with torch.no_grad():\n",
    "        max_next_q_values = self.target_net(next_states).max(1)[0].view(-1, 1)\n",
    "        #print(type(max_next_q_values))\n",
    "        #print(rewards.shape, max_next_q_values.shape, terminateds.shape)\n",
    "        q_targets = rewards + self.gamma * max_next_q_values * (1.0 - terminateds)\n",
    "\n",
    "        dqn_loss = torch.mean(torch.nn.functional.mse_loss(q_values, q_targets))\n",
    "        self.optimizer.zero_grad()\n",
    "        dqn_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.count % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.count += 1\n",
    "\n",
    "\n",
    "hidden_dim = 128\n",
    "\n",
    "lr = 1e-3\n",
    "gamma = 0.98\n",
    "epsilon = 0.1\n",
    "target_update = 5\n",
    "buffer_size = 10000\n",
    "minimal_size = 500\n",
    "learn_freq = 5\n",
    "batch_size = 64\n",
    "device = torch.device(\"cpu\") #torch.device(\"cuda\") if torch.cuda.is_available () else torch.device(\"cpu\")#\n",
    "\n",
    "state_dim = 2\n",
    "action_dim = 3\n",
    "\n",
    "def train(agent, num_episodes):    \n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "    reward_stat = np.zeros(num_episodes)\n",
    "    total_steps = 0\n",
    "    env = MountainCar()\n",
    "    start_time = time.time()\n",
    "    max_reward = -100000 \n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            total_steps += 1\n",
    "            action = agent.take_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            replay_buffer.add(state, action, reward, next_state, terminated)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if replay_buffer.size() > minimal_size:\n",
    "                if total_steps % learn_freq ==0:\n",
    "                    states, actions, rewards, next_states, terminateds = replay_buffer.sample(batch_size)\n",
    "                    agent.update(states, actions, rewards, next_states, terminateds)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        new_max_reard = max_reward < total_reward\n",
    "        if new_max_reard:\n",
    "            max_reward = total_reward\n",
    "        if new_max_reard or episode*10 % num_episodes == 0:\n",
    "            duration = time.time() - start_time\n",
    "            steps_per_second = total_steps/duration\n",
    "            print(\"episode:\", episode, \"total_reward:\",total_reward, \"step/second:\", steps_per_second)\n",
    "        reward_stat[episode] = total_reward\n",
    "    return total_steps,reward_stat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 total_reward: -500 step/second: 6943.9819873514125\n",
      "episode: 21 total_reward: -178 step/second: 2216.9925079171976\n",
      "episode: 24 total_reward: -167 step/second: 2207.5811313796416\n",
      "episode: 49 total_reward: -155 step/second: 2180.628862283593\n",
      "episode: 50 total_reward: -151 step/second: 2179.874582507231\n",
      "episode: 64 total_reward: -149 step/second: 2181.007656217816\n",
      "episode: 71 total_reward: -127 step/second: 2178.060958771803\n",
      "episode: 100 total_reward: -203 step/second: 2176.333695197174\n",
      "episode: 153 total_reward: -117 step/second: 2171.4476709153605\n",
      "episode: 155 total_reward: -109 step/second: 2171.8503424436244\n",
      "episode: 169 total_reward: -105 step/second: 2175.086766497154\n",
      "episode: 200 total_reward: -165 step/second: 2174.386615527745\n",
      "episode: 201 total_reward: -94 step/second: 2174.859771037765\n",
      "episode: 244 total_reward: -90 step/second: 2179.281747455768\n",
      "episode: 300 total_reward: -267 step/second: 2185.602923682547\n",
      "episode: 349 total_reward: -86 step/second: 2190.554532682087\n",
      "episode: 400 total_reward: -135 step/second: 2191.3496424064856\n",
      "episode: 500 total_reward: -141 step/second: 2186.2504793876215\n",
      "episode: 600 total_reward: -142 step/second: 2187.057026976425\n",
      "episode: 700 total_reward: -477 step/second: 2188.622857386702\n",
      "episode: 800 total_reward: -146 step/second: 2191.6288826600626\n",
      "episode: 900 total_reward: -255 step/second: 2189.3978037388924\n",
      "245092\n"
     ]
    }
   ],
   "source": [
    "agent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon, target_update, device)\n",
    "total_steps, reward_stat = train(agent, 1000)\n",
    "print(total_steps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
