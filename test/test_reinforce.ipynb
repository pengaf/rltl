{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def clamp(value, min_value, max_value):\n",
    "    return max(min(value, max_value), min_value)\n",
    "\n",
    "class MountainCar:\n",
    "    def __init__(self,  maxStep=500):\n",
    "        self.maxStep = maxStep\n",
    "        self.curStep = 0\n",
    "        self.pxbound = (-1.2, 0.5)\n",
    "        self.vxbound = (-0.07, 0.07)\n",
    "\n",
    "    def state_space(self):\n",
    "        return (self.pxbound, self.vxbound)\n",
    "    \n",
    "    def action_shape(self):\n",
    "        return 3\n",
    "    \n",
    "    def reset(self):\n",
    "        self.curStep = 0\n",
    "        self.px = random.random()*0.2 - 0.6        \n",
    "        self.vx = 0\n",
    "        return (self.px, self.vx), \"\"\n",
    "    \n",
    "    def step(self, action):\n",
    "        #assert(0 <= action and action <=2)\n",
    "        acc = (action - 1.0)*0.001\n",
    "        self.vx = clamp(self.vx + acc - 0.0025*math.cos(3 * self.px), *self.vxbound)\n",
    "        self.px = clamp(self.px + self.vx, *self.pxbound)\n",
    "        if self.px == self.pxbound[0]:\n",
    "            self.vx = 0\n",
    "        terminated = self.px == self.pxbound[1]\n",
    "        self.curStep += 1\n",
    "        truncated = self.curStep >= self.maxStep\n",
    "        return (self.px, self.vx), -1, terminated, truncated, \"\"\n",
    "\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        #x = torch.nn.functional.log_softmax(self.fc2(x), 1)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class REINFORCE:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, device):\n",
    "        self.policy_net = PolicyNet(state_dim, hidden_dim, action_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr = learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = torch.nn.functional.softmax(self.policy_net(state),dim=1)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, states, actions, rewards):\n",
    "        self.optimizer.zero_grad()\n",
    "        g = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            state = torch.tensor(states[i],dtype=torch.float).view(1,-1).to(self.device)\n",
    "            action = torch.tensor(actions[i]).view(1,-1).to(self.device)\n",
    "            reward = rewards[i]\n",
    "            #output = self.policy_net(state)\n",
    "            output = torch.nn.functional.log_softmax(self.policy_net(state),dim=1)            \n",
    "            log_prob = output.gather(1, action)\n",
    "            g = g * self.gamma + reward\n",
    "            loss = log_prob * (-g * math.pow(self.gamma, i))\n",
    "            loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    '''def update(self, states, actions, rewards):\n",
    "        count = len(rewards)\n",
    "        ngs = torch.zeros(count)\n",
    "        g = 0\n",
    "        for i in reversed(range(count)):\n",
    "            g = g * self.gamma + rewards[i]\n",
    "            ngs[i] = -g \n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(actions).view(count, -1).to(self.device)\n",
    "        ngs = ngs.view(count, -1).to(device)\n",
    "        \n",
    "        outputs = torch.nn.functional.log_softmax(self.policy_net(states),dim=1)\n",
    "        #print(outputs.shape, actions.shape, ngs.shape)\n",
    "        #loss = torch.sum(torch.log(outputs.gather(1, actions)) * ngs)\n",
    "        loss = torch.sum(outputs.gather(1, actions) * ngs)\n",
    "        #loss = (torch.nn.functional.cross_entropy(outputs.gather(1, actions), actions.to(torch.float))*ngs).sum()\n",
    "        #loss = torch.nn.functional.cross_entropy(outputs, actions) * ngs\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()'''\n",
    "\n",
    "hidden_dim = 128\n",
    "\n",
    "lr = 1e-3\n",
    "gamma = 0.98\n",
    "device = torch.device(\"cpu\") #torch.device(\"cuda\") if torch.cuda.is_available () else torch.device(\"cpu\")\n",
    "state_dim = 2\n",
    "action_dim = 3\n",
    "\n",
    "\n",
    "def train(agent, num_episodes):    \n",
    "    reward_stat = np.zeros(num_episodes)\n",
    "    env = MountainCar()\n",
    "    start_time = time.time()\n",
    "    total_steps = 0\n",
    "    max_reward = -100000 \n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        total_reward = 0\n",
    "        states=[]\n",
    "        actions=[]\n",
    "        rewards=[]\n",
    "        while True:\n",
    "            total_steps += 1\n",
    "            action = agent.take_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        agent.update(states,actions,rewards)\n",
    "        new_max_reard = max_reward < total_reward\n",
    "        if new_max_reard:\n",
    "            max_reward = total_reward\n",
    "        if new_max_reard or episode*10 % num_episodes == 0:\n",
    "            duration = time.time() - start_time\n",
    "            steps_per_second = total_steps/duration\n",
    "            print(\"episode:\", episode, \"total_reward:\",total_reward, \"step/second:\", steps_per_second)\n",
    "        reward_stat[episode] = total_reward\n",
    "    return total_steps,reward_stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 total_reward: -500 step/second: 1744.4169394979904\n",
      "episode: 100 total_reward: -500 step/second: 1640.192907960816\n",
      "episode: 200 total_reward: -500 step/second: 1629.417752308192\n",
      "episode: 300 total_reward: -500 step/second: 1617.3999043185195\n",
      "episode: 400 total_reward: -500 step/second: 1611.597480479977\n",
      "episode: 500 total_reward: -500 step/second: 1609.6150735223455\n",
      "episode: 600 total_reward: -500 step/second: 1611.693696183803\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\pengaf\\rltl\\test\\test_reinforce.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_reinforce.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m agent \u001b[39m=\u001b[39m REINFORCE(state_dim, hidden_dim, action_dim, lr, gamma, device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_reinforce.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m total_steps, reward_stat \u001b[39m=\u001b[39m train(agent, \u001b[39m1000\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_reinforce.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(total_steps)\n",
      "\u001b[1;32mg:\\pengaf\\rltl\\test\\test_reinforce.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_reinforce.ipynb#W1sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_reinforce.ipynb#W1sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m     total_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_reinforce.ipynb#W1sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtake_action(state)\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_reinforce.ipynb#W1sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m     next_state, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_reinforce.ipynb#W1sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m     states\u001b[39m.\u001b[39mappend(state)\n",
      "\u001b[1;32mg:\\pengaf\\rltl\\test\\test_reinforce.ipynb Cell 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_reinforce.ipynb#W1sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_net(state),dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_reinforce.ipynb#W1sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m action_dist \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mCategorical(probs)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_reinforce.ipynb#W1sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m action \u001b[39m=\u001b[39m action_dist\u001b[39m.\u001b[39;49msample()\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/pengaf/rltl/test/test_reinforce.ipynb#W1sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mreturn\u001b[39;00m action\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\distributions\\categorical.py:132\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[1;34m(self, sample_shape)\u001b[0m\n\u001b[0;32m    130\u001b[0m     sample_shape \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mSize(sample_shape)\n\u001b[0;32m    131\u001b[0m probs_2d \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobs\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_events)\n\u001b[1;32m--> 132\u001b[0m samples_2d \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmultinomial(probs_2d, sample_shape\u001b[39m.\u001b[39;49mnumel(), \u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mT\n\u001b[0;32m    133\u001b[0m \u001b[39mreturn\u001b[39;00m samples_2d\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = REINFORCE(state_dim, hidden_dim, action_dim, lr, gamma, device)\n",
    "total_steps, reward_stat = train(agent, 1000)\n",
    "print(total_steps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
